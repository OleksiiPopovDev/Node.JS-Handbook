### Як зчитувати великі файли за допомогою Node.js?

Для зчитування великих файлів у Node.js ефективно використовуються **потоки (streams)**. Потоки дозволяють зчитувати дані по частинах, не завантажуючи весь файл у пам'ять, що є важливим для обробки великих файлів.

#### **Способи зчитування великих файлів**

---

##### **1. Використання потоків (`fs.createReadStream`)**
Потоки дозволяють читати файл частинами, що зменшує споживання пам'яті.

```javascript
const fs = require('fs');

const stream = fs.createReadStream('large-file.txt', { encoding: 'utf8' });

stream.on('data', (chunk) => {
  console.log('Частина даних:', chunk);
});

stream.on('end', () => {
  console.log('Читання завершено.');
});

stream.on('error', (err) => {
  console.error('Помилка під час читання файлу:', err);
});
```
#### Особливості:
* Дані обробляються частинами (chunks).
* Не блокує основний потік.

---
##### **2. Використання модулів для роботи з потоками**

Node.js надає додаткові модулі для зручної роботи з потоками, наприклад, readline для обробки файлу построково.
```javascript
const fs = require('fs');
const readline = require('readline');

const rl = readline.createInterface({
  input: fs.createReadStream('large-file.txt'),
  output: process.stdout,
  terminal: false,
});

rl.on('line', (line) => {
  console.log('Рядок:', line);
});

rl.on('close', () => {
  console.log('Файл повністю зчитано.');
});
```
#### Особливості:
* Підходить для обробки великих текстових файлів построково.
* Зручно для обробки логів або CSV-файлів.

---
##### **3. Зчитування великих файлів у буфері (fs.read)**

Цей підхід дозволяє контролювати розмір частин, які читаються, використовуючи метод fs.open і fs.read.
```javascript
const fs = require('fs');

fs.open('large-file.txt', 'r', (err, fd) => {
  if (err) throw err;

  const buffer = Buffer.alloc(1024); // Розмір буфера: 1024 байти

  function readNextChunk() {
    fs.read(fd, buffer, 0, buffer.length, null, (err, bytesRead) => {
      if (err) throw err;

      if (bytesRead > 0) {
        console.log('Частина даних:', buffer.toString('utf8', 0, bytesRead));
        readNextChunk(); // Зчитування наступної частини
      } else {
        fs.close(fd, () => console.log('Читання завершено.'));
      }
    });
  }

  readNextChunk();
});
```
#### Особливості:
* Більший контроль над процесом читання.
* Можливість читати файл блоками певного розміру.

---
**Чому краще використовувати потоки для великих файлів?**
- Економія пам’яті:
- - Потоки зчитують файл частинами, зменшуючи ризик перевантаження пам’яті.
- Масштабованість:
- -	Потоки дозволяють обробляти великі файли, незалежно від їхнього розміру.
- Асинхронність:
- -	Потоки не блокують основний потік виконання програми.

#### **Висновок**
Для роботи з великими файлами в Node.js рекомендується використовувати потоки (fs.createReadStream) або модуль readline для построкової обробки. Це забезпечує ефективність і запобігає перевантаженню пам’яті.


| Попереднє питання                                                                                     | Наступне питання |
|-------------------------------------------------------------------------------------------------------|------------------|
| [Node.js інтерпретує чи компілює код програми?](7-does-node-js-interpret-or-compile-program-code.md)  |                  |
